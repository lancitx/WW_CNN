import random
import numpy as np
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv1D, GlobalMaxPooling1D, Dense, Dropout
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import r2_score, mean_absolute_percentage_error
from sklearn.model_selection import train_test_split
from tqdm import tqdm
import matplotlib.pyplot as plt
import matplotlib
import tensorflow as tf
import warnings
import os

# æŠ‘åˆ¶TensorFlowè­¦å‘Š
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
tf.get_logger().setLevel('ERROR')
warnings.filterwarnings('ignore')


# è®¾ç½®éšæœºç§å­
def set_seeds(seed=42):
    np.random.seed(seed)
    random.seed(seed)
    tf.random.set_seed(seed)


set_seeds(42)

# é…ç½®matplotlibå­—ä½“
matplotlib.rcParams['font.sans-serif'] = ['DejaVu Sans', 'Arial']
matplotlib.rcParams['axes.unicode_minus'] = False

# -----------------------------
# 1. æ•°æ®å‡†å¤‡ä¸é¢„å¤„ç†
# -----------------------------
data = np.array([
    [5.33, 5.39, 5.29, 5.41, 5.45, 5.50],
    [5.39, 5.29, 5.41, 5.50, 5.57, 5.57],
    [5.29, 5.41, 5.45, 5.50, 5.57, 5.58],
    [5.41, 5.45, 5.50, 5.57, 5.58, 5.61],
    [5.45, 5.50, 5.57, 5.58, 5.61, 5.69],
    [5.50, 5.57, 5.58, 5.61, 5.69, 5.78],
    [5.57, 5.58, 5.61, 5.69, 5.78, 5.78],
    [5.58, 5.61, 5.69, 5.78, 5.78, 5.81],
    [5.61, 5.69, 5.78, 5.78, 5.81, 5.86],
    [5.69, 5.78, 5.78, 5.81, 5.86, 5.90],
    [5.78, 5.78, 5.81, 5.86, 5.90, 5.97],
    [5.78, 5.81, 5.86, 5.90, 5.97, 6.49],
    [5.81, 5.86, 5.90, 5.97, 6.49, 6.60],
    [5.86, 5.90, 5.97, 6.49, 6.60, 6.64],
    [5.90, 5.97, 6.49, 6.60, 6.64, 6.74],
    [5.97, 6.49, 6.60, 6.64, 6.74, 6.87],
    [6.49, 6.60, 6.64, 6.74, 6.87, 7.01]
])

X = data[:, :5]
y = data[:, 5]

# è®­ç»ƒé›†ï¼šå‰14æ¡ï¼›æµ‹è¯•é›†ï¼šå3æ¡
X_train = X[:14, :]
y_train = y[:14]
X_test = X[14:, :]
y_test_true = y[14:]

# å½’ä¸€åŒ–/æ ‡å‡†åŒ–
scaler_X = StandardScaler()
X_train_scaled = scaler_X.fit_transform(X_train)
X_test_scaled = scaler_X.transform(X_test)

scaler_y = StandardScaler()
y_train_scaled = scaler_y.fit_transform(y_train.reshape(-1, 1)).flatten()

# é‡å¡‘ä¸ºCNNè¾“å…¥æ ¼å¼
X_train_3d = X_train_scaled.reshape((X_train_scaled.shape[0], X_train_scaled.shape[1], 1))
X_test_3d = X_test_scaled.reshape((X_test_scaled.shape[0], X_test_scaled.shape[1], 1))


# -----------------------------
# 2. æ¨¡å‹æ„å»ºå‡½æ•°ï¼ˆä¼˜åŒ–ç‰ˆï¼‰
# -----------------------------
def build_cnn_model(hyperparams):
    """æ„å»ºCNNæ¨¡å‹ï¼Œä¸WOAä¿æŒä¸€è‡´çš„5ä¸ªå‚æ•°"""
    f1 = max(4, min(32, int(round(hyperparams[0]))))  # é™åˆ¶èŒƒå›´
    f2 = max(2, min(16, int(round(hyperparams[1]))))
    fc = max(4, min(32, int(round(hyperparams[2]))))
    lr = max(1e-4, min(1e-2, float(hyperparams[3])))
    dropout = max(0.1, min(0.4, float(hyperparams[4])))

    model = Sequential([
        Conv1D(filters=f1, kernel_size=2, activation='relu', padding='same', input_shape=(5, 1)),
        Conv1D(filters=f2, kernel_size=2, activation='relu', padding='same'),
        GlobalMaxPooling1D(),
        Dense(fc, activation='relu'),
        Dropout(dropout),
        Dense(1)
    ])

    optimizer = Adam(learning_rate=lr)
    model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])
    return model


# -----------------------------
# 3. ç®€åŒ–çš„é€‚åº”åº¦å‡½æ•°ï¼ˆä¸WOAä¸€è‡´ï¼‰
# -----------------------------
def fitness_function_simplified(hyperparams):
    """
    ç®€åŒ–çš„é€‚åº”åº¦å‡½æ•°ï¼Œä½¿ç”¨ç•™å‡ºéªŒè¯è€Œéäº¤å‰éªŒè¯ï¼Œå‡å°‘æ¨¡å‹åˆ›å»ºæ¬¡æ•°
    """
    # å‚æ•°èŒƒå›´æ£€æŸ¥
    f1, f2, fc, lr, dropout = hyperparams
    if not (4 <= f1 <= 32 and 2 <= f2 <= 16 and 4 <= fc <= 32 and 1e-4 <= lr <= 1e-2 and 0.1 <= dropout <= 0.4):
        return 1e6

    try:
        # ä½¿ç”¨ç®€å•çš„ç•™å‡ºéªŒè¯
        X_tr, X_val, y_tr, y_val = train_test_split(
            X_train_3d, y_train_scaled, test_size=0.25, random_state=42, shuffle=True
        )

        # æ„å»ºå¹¶è®­ç»ƒæ¨¡å‹
        model = build_cnn_model(hyperparams)

        # çŸ­æ—¶é—´è®­ç»ƒï¼Œé¿å…è¿‡åº¦æ‹Ÿåˆ
        history = model.fit(
            X_tr, y_tr,
            epochs=15,  # å‡å°‘è®­ç»ƒè½®æ•°
            batch_size=2,
            verbose=0,
            validation_data=(X_val, y_val)
        )

        # è·å–æœ€åå‡ ä¸ªepochçš„å¹³å‡éªŒè¯æŸå¤±
        val_losses = history.history['val_loss']
        avg_val_loss = np.mean(val_losses[-3:])  # æœ€å3ä¸ªepochçš„å¹³å‡

        # é¢„æµ‹éªŒè¯é›†è®¡ç®—RÂ²
        y_pred_scaled = model.predict(X_val, verbose=0)
        y_pred = scaler_y.inverse_transform(y_pred_scaled).flatten()
        y_true = scaler_y.inverse_transform(y_val.reshape(-1, 1)).flatten()

        r2 = r2_score(y_true, y_pred)

        # å¤æ‚åº¦æƒ©ç½š
        complexity = (f1 + f2 + fc) / 100

        # ç»¼åˆå¾—åˆ†ï¼šéªŒè¯æŸå¤± + RÂ²æƒ©ç½š + å¤æ‚åº¦æƒ©ç½š
        score = avg_val_loss + 0.3 * max(0, 1 - r2) + 0.05 * complexity

        # æ¸…ç†æ¨¡å‹ï¼Œé‡Šæ”¾å†…å­˜
        del model
        tf.keras.backend.clear_session()

        return float(score)

    except Exception as e:
        print(f"é€‚åº”åº¦è¯„ä¼°å‡ºé”™: {e}")
        return 1e6


# -----------------------------
# 4. BWOç®—æ³•ï¼ˆç»Ÿä¸€é£æ ¼ï¼‰
# -----------------------------
def beluga_whale_optimization_unified(n_whales=6, max_iter=8):
    """
    BWOç®—æ³•ï¼Œç»Ÿä¸€æ‰“å°é£æ ¼
    """
    # 5ç»´å‚æ•°ï¼š[f1, f2, fc, lr, dropout]
    lb = np.array([4, 2, 4, 1e-4, 0.1])
    ub = np.array([32, 16, 32, 1e-2, 0.4])

    dim = 5

    # åˆå§‹åŒ–ç™½é²¸ç¾¤ä½“
    X = np.random.uniform(low=lb, high=ub, size=(n_whales, dim))
    fitness = np.full(n_whales, np.inf)

    # è®°å½•å†å²
    best_fitness_history = []

    print("ğŸ³ åˆå§‹åŒ–ç™½é²¸ç¾¤ä½“...")
    # è¯„ä¼°åˆå§‹ç¾¤ä½“
    for i in range(n_whales):
        fitness[i] = fitness_function_simplified(X[i])
        print(f"ç™½é²¸ {i + 1}: é€‚åº”åº¦ = {fitness[i]:.4f}")

    # æ‰¾åˆ°åˆå§‹æœ€ä¼˜
    best_idx = np.argmin(fitness)
    X_best = X[best_idx].copy()
    best_fit = fitness[best_idx]
    best_fitness_history.append(best_fit)

    print(f"åˆå§‹æœ€ä¼˜é€‚åº”åº¦: {best_fit:.6f}")

    # è¿­ä»£ä¼˜åŒ–
    for t in tqdm(range(max_iter), desc="ğŸ” BWOä¼˜åŒ–è¿›åº¦"):
        # Beluga Whale ç‰¹æœ‰å‚æ•°
        a = 2 - 2 * (t / max_iter)

        improved_count = 0
        for i in range(n_whales):
            r1 = random.random()
            r2 = random.random()
            A = 2 * a * r1 - a
            C = 2 * r2
            b = 1
            l = random.uniform(-1, 1)
            p = random.random()

            if p < 0.5:
                # Beluga"åŒ…å›´çŒç‰©"/"éšæœºæœç´¢"æœºåˆ¶
                if abs(A) < 1:
                    # é è¿‘å½“å‰å…¨å±€æœ€ä¼˜
                    D = np.abs(C * X_best - X[i])
                    X_new = X_best - A * D
                else:
                    # é€‰ä¸€åªéšæœºç™½é²¸è¿›è¡Œæ¢ç´¢
                    rand_idx = random.randint(0, n_whales - 1)
                    X_rand = X[rand_idx]
                    D = np.abs(C * X_rand - X[i])
                    X_new = X_rand - A * D
            else:
                # Beluga"èºæ—‹æ›´æ–°"æœºåˆ¶
                D_best = np.abs(X_best - X[i])
                X_new = D_best * np.exp(b * l) * np.cos(2 * np.pi * l) + X_best

            # è¾¹ç•Œå¤„ç†
            X_new = np.clip(X_new, lb, ub)

            # è¯„ä¼°æ–°ä½ç½®
            f_new = fitness_function_simplified(X_new)

            # æ›´æ–°ä½ç½®
            if f_new < fitness[i]:
                X[i] = X_new.copy()
                fitness[i] = f_new
                improved_count += 1

                # æ›´æ–°å…¨å±€æœ€ä¼˜
                if f_new < best_fit:
                    X_best = X_new.copy()
                    best_fit = f_new

        best_fitness_history.append(best_fit)
        print(f"ç¬¬ {t + 1} ä»£: æœ€ä¼˜é€‚åº”åº¦ = {best_fit:.6f}, æ”¹è¿› {improved_count} ä¸ªä¸ªä½“")

    return X_best, best_fit, best_fitness_history


# -----------------------------
# 5. æ¨¡å‹è¯„ä¼°å‡½æ•°ï¼ˆä¸WOAä¸€è‡´ï¼‰
# -----------------------------
def evaluate_final_model(model, X_train_3d, X_test_3d, y_train, y_test, scaler_y):
    """è¯„ä¼°æœ€ç»ˆæ¨¡å‹"""
    # è®­ç»ƒé›†é¢„æµ‹
    y_train_pred_scaled = model.predict(X_train_3d, verbose=0)
    y_train_pred = scaler_y.inverse_transform(y_train_pred_scaled).flatten()

    # æµ‹è¯•é›†é¢„æµ‹
    y_test_pred_scaled = model.predict(X_test_3d, verbose=0)
    y_test_pred = scaler_y.inverse_transform(y_test_pred_scaled).flatten()

    # è®¡ç®—æŒ‡æ ‡
    train_r2 = r2_score(y_train, y_train_pred)
    train_mse = np.mean((y_train_pred - y_train) ** 2)
    train_mae = np.mean(np.abs(y_train_pred - y_train))

    test_r2 = r2_score(y_test, y_test_pred)
    test_mse = np.mean((y_test_pred - y_test) ** 2)
    test_mae = np.mean(np.abs(y_test_pred - y_test))

    return {
        'y_train_pred': y_train_pred, 'y_test_pred': y_test_pred,
        'train_r2': train_r2, 'train_mse': train_mse, 'train_mae': train_mae,
        'test_r2': test_r2, 'test_mse': test_mse, 'test_mae': test_mae
    }


# -----------------------------
# 6. å¯è§†åŒ–å‡½æ•°ï¼ˆä¸WOAä¸€è‡´ï¼‰
# -----------------------------
def plot_results_clean(history_opt, bwo_history, y_train, y_test_true, results, best_params):
    """æ¸…æ™°çš„ç»“æœå¯è§†åŒ–ï¼Œä¸WOAä¿æŒä¸€è‡´"""
    fig, axes = plt.subplots(2, 2, figsize=(12, 10))  # å‡å°å›¾å½¢å°ºå¯¸

    # 1. BWOä¼˜åŒ–è¿‡ç¨‹
    ax1 = axes[0, 0]
    ax1.plot(range(len(bwo_history)), bwo_history, 'b-o', linewidth=2, markersize=6)
    ax1.set_title('BWO Optimization Process', fontsize=14, fontweight='bold')
    ax1.set_xlabel('Iteration')
    ax1.set_ylabel('Fitness Value')
    ax1.grid(True, alpha=0.3)
    ax1.text(0.7, 0.8, f'Best: {min(bwo_history):.4f}', transform=ax1.transAxes,
             bbox=dict(boxstyle="round,pad=0.3", facecolor="lightblue"))

    # 2. è®­ç»ƒè¿‡ç¨‹
    ax2 = axes[0, 1]
    ax2.plot(history_opt.history['loss'], label='Training Loss', linewidth=2, color='blue')
    if 'val_loss' in history_opt.history:
        ax2.plot(history_opt.history['val_loss'], label='Validation Loss', linewidth=2, color='red', linestyle='--')
    ax2.set_title('Model Training Process', fontsize=14, fontweight='bold')
    ax2.set_xlabel('Epoch')
    ax2.set_ylabel('MSE')
    ax2.legend()
    ax2.grid(True, alpha=0.3)

    # 3. é¢„æµ‹ç»“æœå¯¹æ¯”
    ax3 = axes[1, 0]
    train_indices = range(1, len(y_train) + 1)
    test_indices = range(len(y_train) + 1, len(y_train) + len(y_test_true) + 1)

    ax3.plot(train_indices, y_train, 'bo-', label='Train True', markersize=8, linewidth=2)
    ax3.plot(train_indices, results['y_train_pred'], 'b^--', label='Train Pred',
             alpha=0.8, markersize=8, linewidth=2)
    ax3.plot(test_indices, y_test_true, 'ro-', label='Test True', markersize=10, linewidth=3)
    ax3.plot(test_indices, results['y_test_pred'], 'rs--', label='Test Pred',
             alpha=0.8, markersize=10, linewidth=3)

    ax3.set_title(f'Prediction Results (Train R2={results["train_r2"]:.3f}, Test R2={results["test_r2"]:.3f})',
                  fontsize=14, fontweight='bold')
    ax3.set_xlabel('Sample Number')
    ax3.set_ylabel('Mileage (10^4 km)')
    ax3.legend()
    ax3.grid(True, alpha=0.3)

    # 4. æ•£ç‚¹å›¾
    ax4 = axes[1, 1]
    ax4.scatter(y_train, results['y_train_pred'], color='blue', alpha=0.8, s=100,
                label=f'Train (R2={results["train_r2"]:.3f})')
    ax4.scatter(y_test_true, results['y_test_pred'], color='red', alpha=0.8, s=150,
                label=f'Test (R2={results["test_r2"]:.3f})')

    # ç†æƒ³çº¿
    all_values = np.concatenate([y_train, y_test_true])
    min_val, max_val = all_values.min() * 0.98, all_values.max() * 1.02
    ax4.plot([min_val, max_val], [min_val, max_val], 'k--', alpha=0.5, label='Ideal Prediction')

    ax4.set_title('True vs Predicted', fontsize=14, fontweight='bold')
    ax4.set_xlabel('True Value')
    ax4.set_ylabel('Predicted Value')
    ax4.legend()
    ax4.grid(True, alpha=0.3)

    plt.tight_layout()

    # å°è¯•ä¿å­˜å›¾ç‰‡ï¼Œå¦‚æœå¤±è´¥åˆ™æ˜¾ç¤º
    try:
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        import os
        os.makedirs('../img', exist_ok=True)
        # é™ä½dpiä»¥å‡å°‘å†…å­˜ä½¿ç”¨
        plt.savefig('../img/bwo_cnn_optimized_results.png', dpi=150, bbox_inches='tight')
        print("å›¾è¡¨å·²ä¿å­˜åˆ°: ../img/bwo_cnn_optimized_results.png")
    except Exception as e:
        print(f"ä¿å­˜å›¾ç‰‡å¤±è´¥: {e}")
        print("å°†ç›´æ¥æ˜¾ç¤ºå›¾è¡¨...")

    plt.show()

    # æ‰“å°æœ€ä¼˜å‚æ•°
    print(f"\nğŸ‰ BWOä¼˜åŒ–çš„æœ€ä¼˜è¶…å‚æ•°:")
    print(f"   Conv1æ»¤æ³¢å™¨: {int(best_params[0])}")
    print(f"   Conv2æ»¤æ³¢å™¨: {int(best_params[1])}")
    print(f"   å…¨è¿æ¥å•å…ƒ: {int(best_params[2])}")
    print(f"   å­¦ä¹ ç‡: {best_params[3]:.5f}")
    print(f"   Dropoutç‡: {best_params[4]:.3f}")


# -----------------------------
# 7. ä¸»æµç¨‹
# -----------------------------
if __name__ == "__main__":
    print("=== ğŸš€ ä¼˜åŒ–ç‰ˆ BWO-CNN è¶…å‚æ•°æœç´¢ç³»ç»Ÿ ===\n")

    # 1. BWOè¶…å‚æ•°ä¼˜åŒ–
    print("å¼€å§‹BWOè¶…å‚æ•°ä¼˜åŒ–...")
    best_params, best_val_score, bwo_history = beluga_whale_optimization_unified(n_whales=5, max_iter=6)

    print(f"\nâœ… BWOä¼˜åŒ–å®Œæˆ!")
    f1_opt, f2_opt, fc_opt, lr_opt, dropout_opt = best_params
    print(f"æœ€ä¼˜è¶…å‚æ•°ç»„åˆ:")
    print(f"  - Conv1æ»¤æ³¢å™¨: {int(f1_opt)}")
    print(f"  - Conv2æ»¤æ³¢å™¨: {int(f2_opt)}")
    print(f"  - å…¨è¿æ¥å•å…ƒ: {int(fc_opt)}")
    print(f"  - å­¦ä¹ ç‡: {lr_opt:.5f}")
    print(f"  - Dropoutç‡: {dropout_opt:.3f}")
    print(f"  - æœ€ä¼˜é€‚åº”åº¦: {best_val_score:.6f}")

    # 2. è®­ç»ƒæœ€ç»ˆæ¨¡å‹
    print(f"\nğŸ‹ï¸ å¼€å§‹è®­ç»ƒæœ€ç»ˆæ¨¡å‹...")
    model_final = build_cnn_model(best_params)

    # ä½¿ç”¨æ›´é•¿çš„è®­ç»ƒæ—¶é—´æ¥è®­ç»ƒæœ€ç»ˆæ¨¡å‹
    early_stop = EarlyStopping(monitor='loss', patience=20, restore_best_weights=True)
    history_final = model_final.fit(
        X_train_3d, y_train_scaled,
        epochs=100,
        batch_size=2,
        verbose=1,
        callbacks=[early_stop]
    )

    # 3. è¯„ä¼°æ¨¡å‹
    results = evaluate_final_model(model_final, X_train_3d, X_test_3d, y_train, y_test_true, scaler_y)

    # 4. æ‰“å°è¯¦ç»†ç»“æœ
    print(f"\nğŸ“Š === æœ€ç»ˆæ¨¡å‹æ€§èƒ½è¯„ä¼° ===")
    print(f"è®­ç»ƒé›†:")
    print(f"  RÂ² = {results['train_r2']:.4f}")
    print(f"  MSE = {results['train_mse']:.6f}")
    print(f"  MAE = {results['train_mae']:.6f}")

    print(f"\næµ‹è¯•é›†:")
    print(f"  RÂ² = {results['test_r2']:.4f}")
    print(f"  MSE = {results['test_mse']:.6f}")
    print(f"  MAE = {results['test_mae']:.6f}")

    print(f"\nğŸ¯ è¯¦ç»†é¢„æµ‹ç»“æœ:")
    for i, (true_val, pred_val) in enumerate(zip(y_test_true, results['y_test_pred'])):
        error = abs(pred_val - true_val)
        error_pct = (error / true_val) * 100
        print(f"  æ ·æœ¬{i + 15}: çœŸå®={true_val:.4f}, é¢„æµ‹={pred_val:.4f}, "
              f"è¯¯å·®={error:.4f} ({error_pct:.1f}%)")

    # 5. æ€§èƒ½è¯Šæ–­
    overfitting = results['train_r2'] - results['test_r2']
    print(f"\nğŸ” æ¨¡å‹è¯Šæ–­:")
    print(f"  è¿‡æ‹ŸåˆæŒ‡æ ‡: {overfitting:.4f}")
    print(f"  æ¨¡å‹å¤æ‚åº¦: {model_final.count_params()} å‚æ•°")

    if overfitting > 0.2:
        print("  âš ï¸  è½»å¾®è¿‡æ‹Ÿåˆ")
    elif results['test_r2'] > 0.6:
        print("  âœ… æ€§èƒ½ä¼˜ç§€!")
    elif results['test_r2'] > 0.3:
        print("  ğŸ”¶ æ€§èƒ½è‰¯å¥½")
    else:
        print("  ğŸ’¡ å»ºè®®è¿›ä¸€æ­¥ä¼˜åŒ–")

    # 6. å¯è§†åŒ–
    plot_results_clean(history_final, bwo_history, y_train, y_test_true, results, best_params)

    print(f"\nğŸ‰ ä¼˜åŒ–å®Œæˆï¼")